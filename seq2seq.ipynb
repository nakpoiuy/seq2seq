{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mbMPo4s7-EKQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filenum =100\n",
        "text = []\n",
        "files = os.listdir('./randomsong')\n",
        "for file in files[:filenum]:\n",
        "  with open('./randomsong/' + file) as f:\n",
        "    temp = f.read().split('\\n')\n",
        "    text.append(temp)\n",
        "\n",
        "input_text = []\n",
        "target_text = []\n",
        "\n",
        "maxlen=0\n",
        "for t in text:\n",
        "  for i in range(len(t)-1):\n",
        "    input_text.append(t[i] + ' \\n')\n",
        "    target_text.append(' \\t' + t[i+1] + ' \\n')\n",
        "    if maxlen < len(t[i+1].split(' ')):\n",
        "      maxlen = len(t[i+1].split(' '))\n",
        "\n",
        "text = ''\n",
        "files = os.listdir('./randomsong')\n",
        "for file in files[:filenum]:\n",
        "  with open('./randomsong/' + file) as f:\n",
        "    text += f.read()\n",
        "    \n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.split()\n",
        "text += ' \\n \\t'\n",
        "words = sorted(list(set(text)))\n",
        "\n",
        "ind_w = dict([(i,c) for i,c in enumerate(words)])\n",
        "word_i = dict([(c,i) for i,c in enumerate(words)])\n",
        "\n",
        "x = np.zeros((len(input_text), maxlen, len(words)), dtype='float32')\n",
        "y = np.zeros((len(input_text), maxlen, len(words)), dtype='float32')\n",
        "y_next = np.zeros((len(input_text), maxlen, len(words)), dtype='float32')\n",
        "\n",
        "for i,(input, target) in enumerate(zip(input_text, target_text)):\n",
        "  input = input.split()\n",
        "  target = target.split()\n",
        "  for j, (inp,tar) in enumerate(zip(input,target)):\n",
        "    x[i, j, word_i[inp]] = 1\n",
        "    y[i, j, word_i[tar]] = 1\n",
        "    if j > 0:\n",
        "      y_next[i,j-1, word_i[tar]] = 1\n",
        "      \n",
        "\n",
        "encoder_inputs = Input(shape=(None, len(words)))\n",
        "encoder = LSTM(256, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, len(words)))\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "      \n",
        "decoder_dense = Dense(len(words), activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit([x,y], y_next, batch_size=64, epochs=100,validation_split=0.2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UyGOxhIc-HKo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_dim=256\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "\n",
        "def decode_seq(input_seq):\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "  target_seq = np.zeros((1,1, len(words)))\n",
        "  target_seq[0,0, word_i['\\t']] = 1.\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentense = ' '\n",
        "  while not stop_condition:\n",
        "    output_tokens , h, c = decoder_model.predict(\\\n",
        "                         [target_seq] + states_value)\n",
        "    sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
        "    sampled_word = ind_w[sampled_token_index]\n",
        "    decoded_sentense += str(' ') + sampled_word\n",
        "    if (sampled_word == '\\n' or (len(decoded_sentense) > maxlen and target_seq[0,0, word_i['\\t']] != 1)):\n",
        "      stop_condition = True\n",
        "      \n",
        "    target_seq = np.zeros((1,1, len(words)))\n",
        "    target_seq[0,0, sampled_token_index] = 1\n",
        "    \n",
        "    state_value = [h,c]\n",
        "    \n",
        "  return decoded_sentense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rA_VFl7x-JPc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  input_seq = x[i:i+1]\n",
        "  decoded_sentense = decode_seq(input_seq)\n",
        "  \n",
        "  print('\\n' +str(i))\n",
        "  print(input_text[i].replace('\\n', ''))\n",
        "  print(input_text[i+1].replace('\\n', ''))\n",
        "  print(decoded_sentense.replace('\\t', ''))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}